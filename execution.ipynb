{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from prepare_data import preparing, get_word_list, get_embedding, extract_features, final_prepare\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from NLP import extract_nlp_features\n",
    "import Non_NLP\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout,BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate, add, Lambda,multiply, GaussianNoise\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Non_NLP' from '/Users/sizhenhan/Documents/quora-question-pairs/Non_NLP.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(Non_NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preparing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_word_list(X_train)\n",
    "embeddings = get_embedding(words)\n",
    "words = embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1,train2, features_train = extract_features(X_train,words)\n",
    "test1,test2, features_test = extract_features(X_test,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(train1, train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train1, data_train2 = final_prepare(train1,train2,tokenizer)\n",
    "data_test1, data_test2 = final_prepare(test1,test2,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "embedding_matrix = np.zeros((len(words)+1, 300))\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    embedding_matrix[i] = embeddings.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 2., 0., 0.],\n",
       "       [0., 2., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 5., 0., 0.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.copy()\n",
    "X_test2 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nlp_features_train = extract_nlp_features(X_train2)\n",
    "X_nlp_features_test = extract_nlp_features(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_word_min</th>\n",
       "      <th>common_word_max</th>\n",
       "      <th>common_stop_min</th>\n",
       "      <th>common_stop_max</th>\n",
       "      <th>common_token_min</th>\n",
       "      <th>common_token_max</th>\n",
       "      <th>last_word_equal</th>\n",
       "      <th>first_word_equal</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>token_set_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70052</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>0.184211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321015</th>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.285710</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.374995</td>\n",
       "      <td>0.555549</td>\n",
       "      <td>0.333331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268021</th>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.374995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230767</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>0.219178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312808</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265818</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>100</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>83</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        common_word_min  common_word_max  common_stop_min  common_stop_max  \\\n",
       "70052          0.000000         0.000000         0.000000         0.000000   \n",
       "321015         0.499988         0.285710         0.599988         0.374995   \n",
       "268021         0.428565         0.374995         0.000000         0.000000   \n",
       "312808         0.999967         0.999967         0.666644         0.499988   \n",
       "265818         0.999900         0.999900         0.999950         0.666644   \n",
       "\n",
       "        common_token_min  common_token_max  last_word_equal  first_word_equal  \\\n",
       "70052           0.000000          0.000000              0.0               0.0   \n",
       "321015          0.555549          0.333331              0.0               1.0   \n",
       "268021          0.230767          0.199999              0.0               0.0   \n",
       "312808          0.833319          0.714276              1.0               1.0   \n",
       "265818          0.999967          0.749981              1.0               1.0   \n",
       "\n",
       "        abs_len_diff  mean_len  token_set_ratio  token_sort_ratio  fuzz_ratio  \\\n",
       "70052            8.0      11.0               43                43          31   \n",
       "321015           6.0      12.0               60                45          46   \n",
       "268021           2.0      14.0               60                51          28   \n",
       "312808           1.0       6.5               95                85          88   \n",
       "265818           1.0       3.5              100                92          92   \n",
       "\n",
       "        fuzz_partial_ratio  longest_substr_ratio  \n",
       "70052                   52              0.184211  \n",
       "321015                  49              0.180000  \n",
       "268021                  29              0.219178  \n",
       "312808                  81              0.515152  \n",
       "265818                  83              0.615385  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nlp_features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = X_train.copy()\n",
    "X_test3 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nonnlp_features_train, X_nonnlp_features_test = Non_NLP.extract_nonnlp_features(X_train3,X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_kcore</th>\n",
       "      <th>max_kcore</th>\n",
       "      <th>common_neighbor_ratio</th>\n",
       "      <th>common_neighbor_count</th>\n",
       "      <th>min_freq</th>\n",
       "      <th>max_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321015</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312808</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265818</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        min_kcore  max_kcore  common_neighbor_ratio  common_neighbor_count  \\\n",
       "70052           0          0                    0.0                    0.0   \n",
       "321015          0          2                    0.0                    0.0   \n",
       "268021          0          0                    0.0                    0.0   \n",
       "312808          0          2                    0.5                    1.0   \n",
       "265818          0          3                    0.0                    0.0   \n",
       "\n",
       "        min_freq  max_freq  \n",
       "70052          1         5  \n",
       "321015         1         4  \n",
       "268021         1         1  \n",
       "312808         2         2  \n",
       "265818         1         2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nonnlp_features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.hstack((features_train, X_nlp_features_train, X_nonnlp_features_train))\n",
    "features_test = np.hstack((features_test, X_nlp_features_test, X_nonnlp_features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_12 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_11 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 30, 300)      1029600     input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 30, 300)      1029600     input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 75)           112800      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 75)           112800      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 75)           0           lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 75)           0           lstm_8[0][0]                     \n",
      "                                                                 lambda_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 75)           0           add_9[0][0]                      \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 75)           0           lstm_8[0][0]                     \n",
      "                                                                 lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 25)           100         input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 150)          0           multiply_4[0][0]                 \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 200)          5200        batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 150)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 200)          0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 350)          0           dropout_11[0][0]                 \n",
      "                                                                 dropout_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 350)          1400        concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_4 (GaussianNoise (None, 350)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 150)          52650       gaussian_noise_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 150)          0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 150)          600         dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 1)            151         batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,344,901\n",
      "Trainable params: 284,651\n",
      "Non-trainable params: 2,060,250\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(data_train1.shape[1],))\n",
    "embedding1 = Embedding(len(words)+1,300,weights=[embedding_matrix],\n",
    "                       input_length=data_train1.shape[1],trainable=False)(input1)\n",
    "x1 = LSTM(75, recurrent_dropout=0.2)(embedding1)\n",
    "\n",
    "input2 = Input(shape=(data_train1.shape[1],))\n",
    "embedding2 = Embedding(len(words)+1,300,weights=[embedding_matrix],\n",
    "                       input_length=data_train1.shape[1],trainable=False)(input2)\n",
    "x2 = LSTM(75, recurrent_dropout=0.2)(embedding2)\n",
    "\n",
    "input3 = Input(shape=(features_train.shape[1],))\n",
    "dense_feature = BatchNormalization()(input3)\n",
    "dense_feature = Dense(200, activation=\"relu\")(dense_feature)\n",
    "dense_feature = Dropout(0.2)(dense_feature)\n",
    "\n",
    "addition = add([x1, x2])\n",
    "x2_negative = Lambda(lambda x: -x)(x2)\n",
    "subtraction = add([x1, x2_negative])\n",
    "subtraction = multiply([subtraction, subtraction])\n",
    "merged = concatenate([subtraction, addition])\n",
    "merged = Dropout(0.4)(merged)\n",
    "\n",
    "merged = concatenate([merged, dense_feature])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "merged = Dense(150, activation=\"relu\")(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "model = Model(inputs=[input1, input2, input3], outputs=out)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\",metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323432 samples\n",
      "Epoch 1/15\n",
      "323432/323432 [==============================] - 141s 436us/sample - loss: 0.3077 - accuracy: 0.8604\n",
      "Epoch 2/15\n",
      "323432/323432 [==============================] - 133s 412us/sample - loss: 0.2809 - accuracy: 0.8743\n",
      "Epoch 3/15\n",
      "323432/323432 [==============================] - 133s 411us/sample - loss: 0.2738 - accuracy: 0.8774\n",
      "Epoch 4/15\n",
      "323432/323432 [==============================] - 135s 418us/sample - loss: 0.2674 - accuracy: 0.8801\n",
      "Epoch 5/15\n",
      "323432/323432 [==============================] - 135s 417us/sample - loss: 0.2623 - accuracy: 0.8832\n",
      "Epoch 6/15\n",
      "323432/323432 [==============================] - 137s 423us/sample - loss: 0.2594 - accuracy: 0.8841\n",
      "Epoch 7/15\n",
      "323432/323432 [==============================] - 137s 423us/sample - loss: 0.2555 - accuracy: 0.8855\n",
      "Epoch 8/15\n",
      "323432/323432 [==============================] - 140s 432us/sample - loss: 0.2527 - accuracy: 0.8872\n",
      "Epoch 9/15\n",
      "323432/323432 [==============================] - 136s 422us/sample - loss: 0.2493 - accuracy: 0.8886\n",
      "Epoch 10/15\n",
      "323432/323432 [==============================] - 137s 423us/sample - loss: 0.2461 - accuracy: 0.8898\n",
      "Epoch 11/15\n",
      "323432/323432 [==============================] - 140s 433us/sample - loss: 0.2429 - accuracy: 0.8911\n",
      "Epoch 12/15\n",
      "323432/323432 [==============================] - 134s 414us/sample - loss: 0.2397 - accuracy: 0.8926\n",
      "Epoch 13/15\n",
      "323432/323432 [==============================] - 133s 411us/sample - loss: 0.2368 - accuracy: 0.8941\n",
      "Epoch 14/15\n",
      "323432/323432 [==============================] - 134s 416us/sample - loss: 0.2329 - accuracy: 0.8956\n",
      "Epoch 15/15\n",
      "323432/323432 [==============================] - 134s 415us/sample - loss: 0.2304 - accuracy: 0.8971\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a78403128>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_train1, data_train2, features_train], y_train,\n",
    "                     epochs=15, batch_size= 512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([data_test1, data_test2, features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for p in pred:\n",
    "    if p > 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8881990650275792"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8113207547169812"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(preds,y_test,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 30, 300)      1029600     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 30, 300)      1029600     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                  (None, 75)           112800      embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  (None, 75)           112800      embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 75)           0           lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 75)           0           lstm_10[0][0]                    \n",
      "                                                                 lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 75)           0           add_11[0][0]                     \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 75)           0           lstm_10[0][0]                    \n",
      "                                                                 lstm_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 150)          0           multiply_5[0][0]                 \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 150)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 150)          600         dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise_5 (GaussianNoise (None, 150)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 150)          22650       gaussian_noise_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 150)          0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 150)          600         dropout_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 1)            151         batch_normalization_14[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 2,308,801\n",
      "Trainable params: 249,001\n",
      "Non-trainable params: 2,059,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(data_train1.shape[1],))\n",
    "embedding1 = Embedding(len(words)+1,300,weights=[embedding_matrix],\n",
    "                       input_length=data_train1.shape[1],trainable=False)(input1)\n",
    "x1 = LSTM(75, recurrent_dropout=0.2)(embedding1)\n",
    "\n",
    "input2 = Input(shape=(data_train1.shape[1],))\n",
    "embedding2 = Embedding(len(words)+1,300,weights=[embedding_matrix],\n",
    "                       input_length=data_train1.shape[1],trainable=False)(input2)\n",
    "x2 = LSTM(75, recurrent_dropout=0.2)(embedding2)\n",
    "\n",
    "addition = add([x1, x2])\n",
    "x2_negative = Lambda(lambda x: -x)(x2)\n",
    "subtraction = add([x1, x2_negative])\n",
    "subtraction = multiply([subtraction, subtraction])\n",
    "merged = concatenate([subtraction, addition])\n",
    "merged = Dropout(0.4)(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "merged = Dense(150, activation=\"relu\")(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "model2 = Model(inputs=[input1, input2], outputs=out)\n",
    "model2.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\",metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323432 samples\n",
      "Epoch 1/15\n",
      "323432/323432 [==============================] - 134s 414us/sample - loss: 0.6181 - accuracy: 0.6603\n",
      "Epoch 2/15\n",
      "323432/323432 [==============================] - 132s 407us/sample - loss: 0.5471 - accuracy: 0.7201\n",
      "Epoch 3/15\n",
      "323432/323432 [==============================] - 133s 410us/sample - loss: 0.5199 - accuracy: 0.7376\n",
      "Epoch 4/15\n",
      "323432/323432 [==============================] - 133s 410us/sample - loss: 0.4990 - accuracy: 0.7511\n",
      "Epoch 5/15\n",
      "323432/323432 [==============================] - 132s 409us/sample - loss: 0.4832 - accuracy: 0.7607\n",
      "Epoch 6/15\n",
      "323432/323432 [==============================] - 135s 418us/sample - loss: 0.4696 - accuracy: 0.7695\n",
      "Epoch 7/15\n",
      "323432/323432 [==============================] - 131s 404us/sample - loss: 0.4580 - accuracy: 0.7767\n",
      "Epoch 8/15\n",
      "323432/323432 [==============================] - 133s 412us/sample - loss: 0.4467 - accuracy: 0.7834\n",
      "Epoch 9/15\n",
      "323432/323432 [==============================] - 133s 410us/sample - loss: 0.4366 - accuracy: 0.7893\n",
      "Epoch 10/15\n",
      "323432/323432 [==============================] - 134s 416us/sample - loss: 0.4279 - accuracy: 0.7946\n",
      "Epoch 11/15\n",
      "323432/323432 [==============================] - 135s 416us/sample - loss: 0.4201 - accuracy: 0.7995\n",
      "Epoch 12/15\n",
      "323432/323432 [==============================] - 135s 417us/sample - loss: 0.4122 - accuracy: 0.8038\n",
      "Epoch 13/15\n",
      "323432/323432 [==============================] - 136s 419us/sample - loss: 0.4064 - accuracy: 0.8070\n",
      "Epoch 14/15\n",
      "323432/323432 [==============================] - 133s 410us/sample - loss: 0.3999 - accuracy: 0.8113\n",
      "Epoch 15/15\n",
      "323432/323432 [==============================] - 128s 396us/sample - loss: 0.3925 - accuracy: 0.8152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a7f13f2e8>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit([data_train1, data_train2, features_train], y_train,\n",
    "                     epochs=15, batch_size= 512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8011204828217369"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred2 = model2.predict([data_test1, data_test2])\n",
    "preds = []\n",
    "for p in pred2:\n",
    "    if p > 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)\n",
    "accuracy_score(preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6694286379820971"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_score(preds,y_test,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
