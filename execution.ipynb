{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from prepare_data import preparing, get_word_list, get_embedding, extract_features, final_prepare\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from NLP import extract_nlp_features\n",
    "import Non_NLP\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout,BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate, add, Lambda,multiply, GaussianNoise\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Non_NLP' from '/Users/sizhenhan/Documents/quora-question-pairs/Non_NLP.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(Non_NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preparing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = get_word_list(X_train)\n",
    "embeddings = get_embedding(words)\n",
    "words = embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1,train2, features_train = extract_features(X_train,words)\n",
    "test1,test2, features_test = extract_features(X_test,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters=\"\")\n",
    "tokenizer.fit_on_texts(np.append(train1, train2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train1, data_train2 = final_prepare(train1,train2,tokenizer)\n",
    "data_test1, data_test2 = final_prepare(test1,test2,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "embedding_matrix = np.zeros((len(words)+1, 300))\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    embedding_matrix[i] = embeddings.get(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 2., 0., 0.],\n",
       "       [0., 2., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 5., 0., 0.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.copy()\n",
    "X_test2 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nlp_features_train = extract_nlp_features(X_train2)\n",
    "X_nlp_features_test = extract_nlp_features(X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>common_word_min</th>\n",
       "      <th>common_word_max</th>\n",
       "      <th>common_stop_min</th>\n",
       "      <th>common_stop_max</th>\n",
       "      <th>common_token_min</th>\n",
       "      <th>common_token_max</th>\n",
       "      <th>last_word_equal</th>\n",
       "      <th>first_word_equal</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>token_set_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70052</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>43</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>0.184211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321015</th>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.285710</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.374995</td>\n",
       "      <td>0.555549</td>\n",
       "      <td>0.333331</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>0.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268021</th>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.374995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.230767</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>0.219178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312808</th>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.833319</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>95</td>\n",
       "      <td>85</td>\n",
       "      <td>88</td>\n",
       "      <td>81</td>\n",
       "      <td>0.515152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265818</th>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999900</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.999967</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>100</td>\n",
       "      <td>92</td>\n",
       "      <td>92</td>\n",
       "      <td>83</td>\n",
       "      <td>0.615385</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        common_word_min  common_word_max  common_stop_min  common_stop_max  \\\n",
       "70052          0.000000         0.000000         0.000000         0.000000   \n",
       "321015         0.499988         0.285710         0.599988         0.374995   \n",
       "268021         0.428565         0.374995         0.000000         0.000000   \n",
       "312808         0.999967         0.999967         0.666644         0.499988   \n",
       "265818         0.999900         0.999900         0.999950         0.666644   \n",
       "\n",
       "        common_token_min  common_token_max  last_word_equal  first_word_equal  \\\n",
       "70052           0.000000          0.000000              0.0               0.0   \n",
       "321015          0.555549          0.333331              0.0               1.0   \n",
       "268021          0.230767          0.199999              0.0               0.0   \n",
       "312808          0.833319          0.714276              1.0               1.0   \n",
       "265818          0.999967          0.749981              1.0               1.0   \n",
       "\n",
       "        abs_len_diff  mean_len  token_set_ratio  token_sort_ratio  fuzz_ratio  \\\n",
       "70052            8.0      11.0               43                43          31   \n",
       "321015           6.0      12.0               60                45          46   \n",
       "268021           2.0      14.0               60                51          28   \n",
       "312808           1.0       6.5               95                85          88   \n",
       "265818           1.0       3.5              100                92          92   \n",
       "\n",
       "        fuzz_partial_ratio  longest_substr_ratio  \n",
       "70052                   52              0.184211  \n",
       "321015                  49              0.180000  \n",
       "268021                  29              0.219178  \n",
       "312808                  81              0.515152  \n",
       "265818                  83              0.615385  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nlp_features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non NLP features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = X_train.copy()\n",
    "X_test3 = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_nonnlp_features_train, X_nonnlp_features_test = Non_NLP.extract_nonnlp_features(X_train3,X_test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_kcore</th>\n",
       "      <th>max_kcore</th>\n",
       "      <th>common_neighbor_ratio</th>\n",
       "      <th>common_neighbor_count</th>\n",
       "      <th>min_freq</th>\n",
       "      <th>max_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70052</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321015</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268021</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>312808</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265818</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        min_kcore  max_kcore  common_neighbor_ratio  common_neighbor_count  \\\n",
       "70052           0          0                    0.0                    0.0   \n",
       "321015          0          2                    0.0                    0.0   \n",
       "268021          0          0                    0.0                    0.0   \n",
       "312808          0          2                    0.5                    1.0   \n",
       "265818          0          3                    0.0                    0.0   \n",
       "\n",
       "        min_freq  max_freq  \n",
       "70052          1         5  \n",
       "321015         1         4  \n",
       "268021         1         1  \n",
       "312808         2         2  \n",
       "265818         1         2  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nonnlp_features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = np.hstack((features_train, X_nlp_features_train, X_nonnlp_features_train))\n",
    "features_test = np.hstack((features_test, X_nlp_features_test, X_nonnlp_features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_13 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, 30, 300)      1029600     input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_8 (Embedding)         (None, 30, 300)      1029600     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   (None, 75)           112800      embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   (None, 75)           112800      embedding_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 75)           0           lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 75)           0           lstm_8[0][0]                     \n",
      "                                                                 lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_15 (InputLayer)           [(None, 25)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 75)           0           add_4[0][0]                      \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 75)           0           lstm_8[0][0]                     \n",
      "                                                                 lstm_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 25)           100         input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 150)          0           multiply_1[0][0]                 \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 200)          5200        batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 150)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 200)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 350)          0           dropout_5[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 350)          1400        concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "gaussian_noise (GaussianNoise)  (None, 350)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 150)          52650       gaussian_noise[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 150)          0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 150)          600         dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            151         batch_normalization_6[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 2,344,901\n",
      "Trainable params: 284,651\n",
      "Non-trainable params: 2,060,250\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1 = Input(shape=(data_train1.shape[1],))\n",
    "embedding1 = Embedding(len(words)+1,300,weights=[embedding_matrix],\n",
    "                       input_length=data_train1.shape[1],trainable=False)(input1)\n",
    "x1 = LSTM(75, recurrent_dropout=0.2)(embedding1)\n",
    "\n",
    "input2 = Input(shape=(data_train1.shape[1],))\n",
    "embedding2 = Embedding(len(words)+1,300,weights=[embedding_matrix],\n",
    "                       input_length=data_train1.shape[1],trainable=False)(input2)\n",
    "x2 = LSTM(75, recurrent_dropout=0.2)(embedding2)\n",
    "\n",
    "input3 = Input(shape=(features_train.shape[1],))\n",
    "dense_feature = BatchNormalization()(input3)\n",
    "dense_feature = Dense(200, activation=\"relu\")(dense_feature)\n",
    "dense_feature = Dropout(0.2)(dense_feature)\n",
    "\n",
    "addition = add([x1, x2])\n",
    "x2_negative = Lambda(lambda x: -x)(x2)\n",
    "subtraction = add([x1, x2_negative])\n",
    "subtraction = multiply([subtraction, subtraction])\n",
    "merged = concatenate([subtraction, addition])\n",
    "merged = Dropout(0.4)(merged)\n",
    "\n",
    "merged = concatenate([merged, dense_feature])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = GaussianNoise(0.1)(merged)\n",
    "\n",
    "merged = Dense(150, activation=\"relu\")(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "out = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "model = Model(inputs=[input1, input2, input3], outputs=out)\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer=\"nadam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1111 14:46:27.581120 4562367936 deprecation.py:323] From /Users/sizhenhan/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323432 samples\n",
      "Epoch 1/15\n",
      "323432/323432 [==============================] - 125s 386us/sample - loss: 0.3079\n",
      "Epoch 2/15\n",
      "323432/323432 [==============================] - 123s 381us/sample - loss: 0.2818\n",
      "Epoch 3/15\n",
      "323432/323432 [==============================] - 124s 382us/sample - loss: 0.2747\n",
      "Epoch 4/15\n",
      "323432/323432 [==============================] - 125s 387us/sample - loss: 0.2686\n",
      "Epoch 5/15\n",
      "323432/323432 [==============================] - 126s 390us/sample - loss: 0.2643\n",
      "Epoch 6/15\n",
      "323432/323432 [==============================] - 127s 394us/sample - loss: 0.2604\n",
      "Epoch 7/15\n",
      "323432/323432 [==============================] - 129s 398us/sample - loss: 0.2573\n",
      "Epoch 8/15\n",
      "323432/323432 [==============================] - 128s 397us/sample - loss: 0.2543\n",
      "Epoch 9/15\n",
      "323432/323432 [==============================] - 129s 400us/sample - loss: 0.2508\n",
      "Epoch 10/15\n",
      "323432/323432 [==============================] - 130s 401us/sample - loss: 0.2471\n",
      "Epoch 11/15\n",
      "323432/323432 [==============================] - 130s 402us/sample - loss: 0.2428\n",
      "Epoch 12/15\n",
      "323432/323432 [==============================] - 130s 403us/sample - loss: 0.2398\n",
      "Epoch 13/15\n",
      "323432/323432 [==============================] - 131s 404us/sample - loss: 0.2362\n",
      "Epoch 14/15\n",
      "323432/323432 [==============================] - 130s 401us/sample - loss: 0.2338\n",
      "Epoch 15/15\n",
      "323432/323432 [==============================] - 130s 401us/sample - loss: 0.2302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a67ad89b0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([data_train1, data_train2, features_train], y_train,\n",
    "                     epochs=15, batch_size= 512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([data_test1, data_test2, features_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for p in pred:\n",
    "    if p > 0.5:\n",
    "        preds.append(1)\n",
    "    else:\n",
    "        preds.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8884464122288457"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(preds,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
